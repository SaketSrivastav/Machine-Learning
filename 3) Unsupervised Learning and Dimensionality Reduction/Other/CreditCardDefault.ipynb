{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "import sklearn\n",
    "import pylab as pl\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA, FastICA as ICA\n",
    "from collections import defaultdict\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data and Pre Process Train and Test Data Sets to Create Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pandas.read_csv('C://Users//Divya//Desktop//Spring_2019//ML//Project 3//Data//csv//Pre_Process1//CreditCardDefaultTrain.csv')                      \n",
    "\n",
    "#categorical_features_train = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "#continuous_features_train = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "\n",
    "\n",
    "#for col in categorical_features_train:\n",
    "    #dummies = pandas.get_dummies(train[col], prefix=col)\n",
    "    #train = pandas.concat([train, dummies], axis=1)\n",
    "    #train.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "#train.head()\n",
    "#train.to_csv('CreditCardDefaultTrainStd.csv') #Download Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pandas.read_csv('C://Users//Divya//Desktop//Spring_2019//ML//Project 3//Data//csv//Pre_Process1//CreditCardDefaultTest.csv')\n",
    "\n",
    "#categorical_features_test = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "#continuous_features_test = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6']\n",
    "\n",
    "\n",
    "#for col in categorical_features_test:\n",
    "    #dummies = pandas.get_dummies(test[col], prefix=col)\n",
    "    #test = pandas.concat([test, dummies], axis=1)\n",
    "    #test.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "#test.head()\n",
    "#test.to_csv('CreditCardDefaultTestStd.csv') #Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pandas.read_csv('C://Users//Divya//Desktop//Spring_2019//ML//Project 3//Data//csv//Pre_Process2//CreditCardDefaultTrainStd.csv')                      \n",
    "test = pandas.read_csv('C://Users//Divya//Desktop//Spring_2019//ML//Project 3//Data//csv//Pre_Process2//CreditCardDefaultTestStd.csv')\n",
    "\n",
    "#Credit Card Train Data Features and Labels\n",
    "train_data  = train.iloc[:,0:29]\n",
    "train_data = train_data.values\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(train_data)\n",
    "data = mms.transform(train_data)\n",
    "labels = train['defaultPayment_label']\n",
    "\n",
    "\n",
    "#Credit CardTest Data Features and Labels\n",
    "test_data  = test.iloc[:,0:29]\n",
    "test_data = test_data.values\n",
    "mms2 = MinMaxScaler()\n",
    "mms2.fit(test_data)\n",
    "data_test = mms.transform(test_data)\n",
    "labels_test = test['defaultPayment_label']\n",
    "\n",
    "sample_size = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow for Kmeans\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,50)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('K_Means: Credit Card Data Set Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow for Expectation Max\n",
    "silhouette_score = []\n",
    "\n",
    "K = range(2,20)\n",
    "for k in K:\n",
    "    gm = GaussianMixture(n_components= k, n_init=2, random_state=0).fit(data)\n",
    "    gmPredicted = gm.predict(data)\n",
    "    silhouette_score.append(metrics.silhouette_score(data, gmPredicted, metric='euclidean', sample_size=sample_size))\n",
    "    \n",
    "plt.plot(K, silhouette_score, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Expectation Maximization: Credit Card Data Set Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('% 9s     %.2fs       %.3f        %.3f        %.3f        %.3f        %.3f         %.3f       %.3f'\n",
    "          % (name, (time() - t0), \n",
    "             metrics.homogeneity_score(labels, estimator.predict(data)),\n",
    "             metrics.completeness_score(labels, estimator.predict(data)),\n",
    "             metrics.v_measure_score(labels, estimator.predict(data)),\n",
    "             metrics.adjusted_rand_score(labels, estimator.predict(data)),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.predict(data)),\n",
    "             metrics.silhouette_score(data, estimator.predict(data),metric='euclidean',sample_size=sample_size),\n",
    "             float(sum(estimator.predict(data) == labels))/float(len(labels))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_digits_i = [10,20,30,40,50]\n",
    "for i in range(5):\n",
    "    bench_k_means(KMeans(init='k-means++', n_clusters=n_digits_i[i], n_init=10),name=\"k-means++\", data=data)\n",
    "for i in range(5):\n",
    "    bench_k_means(GaussianMixture(n_components=n_digits_i[i],random_state=0),name=\"GaussianMixture\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_data = PCA().fit(data)\n",
    "\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(PCA_data.explained_variance_ratio_*100))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('PCA: Credit Card Dataset Explained Variance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_data = PCA(n_components = 5, whiten=False)\n",
    "PCA_data.fit(data)\n",
    "PCA_data_trans = PCA_data.transform(data)\n",
    "PCA_data_trans_test = PCA_data.transform(data_test)\n",
    "\n",
    "#Plots\n",
    "per_var = np.round(PCA_data.explained_variance_ratio_* 100, decimals=1)\n",
    "print(\"Original Credit Card Data Set Number of Rows and Columns:\", data.shape)\n",
    "print(\"Original Credit Card Data Set Number of Rows and Columns:\", data_test.shape)\n",
    "print(\"PCA Credit Card Train Data Set Number of Rows and Columns:\", PCA_data_trans.shape)\n",
    "print(\"PCA Credit Card Test Data Set Number of Rows and Columns:\", PCA_data_trans_test.shape)\n",
    "\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(PCA_data_trans[:,0],PCA_data_trans[:,1], c = train['defaultPayment_label'])\n",
    "plt.title('PCA: Credit Card Dataset Graph')\n",
    "plt.xlabel('PC1 - {0}%'.format(per_var[0]))\n",
    "plt.ylabel('PC2 - {0}%'.format(per_var[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ICA(X,y,title):\n",
    "    \n",
    "    dims = list(np.arange(2,(X.shape[1]-1),3))\n",
    "    dims.append(X.shape[1])\n",
    "    ica = ICA(random_state=5)\n",
    "    kurt = []\n",
    "\n",
    "    for dim in dims:\n",
    "        ica.set_params(n_components=dim)\n",
    "        tmp = ica.fit_transform(X)\n",
    "        tmp = pandas.DataFrame(tmp)\n",
    "        tmp = tmp.kurt(axis=0)\n",
    "        kurt.append(tmp.abs().mean())\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"ICA Kurtosis: \"+ title)\n",
    "    plt.xlabel(\"Independent Components\")\n",
    "    plt.ylabel(\"Avg Kurtosis Across IC\")\n",
    "    plt.plot(dims, kurt, 'b-')\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "run_ICA(data,labels,\"ICA: Credit Card Dataset Avergae Kurtosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICA_data = FastICA(n_components = 25)\n",
    "ICA_data.fit(data)\n",
    "ICA_data_trans = ICA_data.transform(data)\n",
    "ICA_data_trans_test = ICA_data.transform(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_data = GaussianRandomProjection(n_components=5, eps=0.1)\n",
    "RP_data_trans = RP_data.fit_transform(data)\n",
    "RP_data_trans_test = RP_data.fit_transform(data_test)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(RP_data_trans[:,0],RP_data_trans[:,1], c = train['defaultPayment_label'])\n",
    "plt.title('RP: Credit Card Dataset Graph')\n",
    "plt.xlabel('RP1')\n",
    "plt.ylabel('RP2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run an LDA\n",
    "LDA_data = LinearDiscriminantAnalysis(n_components=1)\n",
    "LDA_data_trans = LDA_data.fit_transform(data, labels)\n",
    "LDA_data_trans_test = LDA_data.fit_transform(data_test, labels_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(LDA_data_trans[:,0], labels, c = train['defaultPayment_label'])\n",
    "plt.title('LDA: Credit Card Dataset Graph')\n",
    "plt.xlabel('LDA1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow for Kmeans\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,30)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(LDA_data_trans)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('K_Means/LDA: Credit Card Data Set Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow for Expectation Max\n",
    "silhouette_score = []\n",
    "\n",
    "K = range(2,10)\n",
    "for k in K:\n",
    "    gm = GaussianMixture(n_components= k, n_init=2, random_state=0).fit(LDA_data_trans)\n",
    "    gmPredicted = gm.predict(LDA_data_trans)\n",
    "    silhouette_score.append(metrics.silhouette_score(LDA_data_trans, gmPredicted, metric='euclidean', sample_size=sample_size))\n",
    "    \n",
    "plt.plot(K, silhouette_score, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Expectation Maximization/LDA: Adult Data Set Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run datasets\n",
    "n_digits_i = [5]\n",
    "for i in range(1):\n",
    "    bench_k_means(KMeans(init='k-means++', n_clusters=n_digits_i[i], n_init=10),name=\"k-means++\", data=LDA_data_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_digits_i = [2]\n",
    "for i in range(1):\n",
    "    bench_k_means(GaussianMixture(n_components=n_digits_i[i],random_state=0),name=\"GaussianMixture\", data=ICA_data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#without transformation\n",
    "#start_time = time.time()\n",
    "data_nn = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(4,), random_state=1, momentum = 0.1, learning_rate_init = 0.1)\n",
    "\n",
    "#4,4\n",
    "#4,4,4,4,4\n",
    "#4,4,4,4,4,4,4,4,4,4\n",
    "#4,4,4,4,4,4,4,4,4,4,4,4,4,4,4 \n",
    "\n",
    "##Test###\n",
    "#data_nn.fit(data, labels)  \n",
    "#data_train_pred = data_nn.predict(data)\n",
    "#float(sum(data_train_pred == labels))/float(len(labels))\n",
    "#print(\"--- %s training seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "##Train###\n",
    "#start_time2 = time.time()\n",
    "#data_test_pred = data_nn.predict(data_test)\n",
    "#float(sum(data_test_pred == labels_test))/float(len(labels_test))\n",
    "#print(\"--- %s testing seconds ---\" % (time.time() - start_time2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Train\n",
    "data_nn.fit(PCA_data_trans, labels)  \n",
    "data_train_pred_PCA = data_nn.predict(PCA_data_trans)\n",
    "float(sum(data_train_pred_PCA == labels))/float(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Test\n",
    "data_test_pred_PCA = data_nn.predict(PCA_data_trans_test)\n",
    "float(sum(data_test_pred_PCA == labels_test))/float(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ICA Train\n",
    "data_nn.fit(ICA_data_trans, labels)  \n",
    "data_train_pred_ICA = data_nn.predict(ICA_data_trans)\n",
    "float(sum(data_train_pred_ICA == labels))/float(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ICA Test\n",
    "data_test_pred_ICA = data_nn.predict(ICA_data_trans_test)\n",
    "float(sum(data_test_pred_ICA == labels_test))/float(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RP Train\n",
    "data_nn.fit(RP_data_trans, labels)  \n",
    "data_train_pred_RP = data_nn.predict(RP_data_trans)\n",
    "float(sum(data_train_pred_RP == labels))/float(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RP Test\n",
    "data_test_pred_RP = data_nn.predict(RP_data_trans_test)\n",
    "float(sum(data_test_pred_RP == labels_test))/float(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Train\n",
    "data_nn.fit(LDA_data_trans, labels)  \n",
    "data_train_pred_LDA = data_nn.predict(LDA_data_trans)\n",
    "float(sum(data_train_pred_LDA == labels))/float(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Test\n",
    "data_test_pred_LDA = data_nn.predict(LDA_data_trans_test)\n",
    "float(sum(data_test_pred_LDA == labels_test))/float(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cluster = KMeans(n_clusters=10)\n",
    "pca_train_dataframe[\"cluster\"] = kmeans_cluster.fit_predict(pca_train_dataframe[pca_train_dataframe.columns[0:]])\n",
    "pca_test_dataframe[\"cluster\"] = kmeans_cluster.fit_predict(pca_test_dataframe[pca_test_dataframe.columns[0:]])\n",
    "\n",
    "kmeans_cluster = KMeans(n_clusters=20)\n",
    "ica_train_dataframe[\"cluster\"] = kmeans_cluster.fit_predict(ica_train_dataframe[ica_train_dataframe.columns[0:]])\n",
    "ica_test_dataframe [\"cluster\"] = kmeans_cluster.fit_predict(ica_test_dataframe [ica_test_dataframe.columns[0:]])\n",
    "\n",
    "kmeans_cluster = KMeans(n_clusters=10)\n",
    "rp_train_dataframe[\"cluster\"] = kmeans_cluster.fit_predict(rp_train_dataframe[rp_train_dataframe.columns[0:]])\n",
    "rp_test_dataframe [\"cluster\"] = kmeans_cluster.fit_predict(rp_test_dataframe [rp_test_dataframe.columns[0:]])\n",
    "\n",
    "kmeans_cluster = KMeans(n_clusters=5)\n",
    "lda_train_dataframe[\"cluster\"] = kmeans_cluster.fit_predict(lda_train_dataframe[lda_train_dataframe.columns[0:]])\n",
    "lda_test_dataframe[\"cluster\"] = kmeans_cluster.fit_predict(lda_test_dataframe [lda_test_dataframe.columns[0:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nn = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(4,), random_state=1, momentum = 0.1, learning_rate_init = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Train\n",
    "data_nn.fit(pca_train_dataframe, labels)  \n",
    "data_train_pred_PCA = data_nn.predict(pca_train_dataframe)\n",
    "float(sum(data_train_pred_PCA == labels))/float(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Test\n",
    "data_test_pred_PCA = data_nn.predict(pca_test_dataframe)\n",
    "float(sum(data_test_pred_PCA == labels_test))/float(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ICA Train\n",
    "data_nn.fit(ica_train_dataframe, labels)  \n",
    "data_train_pred_ICA = data_nn.predict(ica_train_dataframe)\n",
    "float(sum(data_train_pred_ICA == labels))/float(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ICA Test\n",
    "data_test_pred_ICA = data_nn.predict(ica_test_dataframe)\n",
    "float(sum(data_test_pred_ICA == labels_test))/float(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RP Train\n",
    "data_nn.fit(rp_train_dataframe, labels)  \n",
    "data_train_pred_RP = data_nn.predict(rp_train_dataframe)\n",
    "float(sum(data_train_pred_RP == labels))/float(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RP Test\n",
    "data_test_pred_RP = data_nn.predict(rp_test_dataframe)\n",
    "float(sum(data_test_pred_RP == labels_test))/float(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Train\n",
    "data_nn.fit(lda_train_dataframe, labels)  \n",
    "data_train_pred_LDA = data_nn.predict(lda_train_dataframe)\n",
    "float(sum(data_train_pred_LDA == labels))/float(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Test\n",
    "data_test_pred_LDA = data_nn.predict(lda_test_dataframe)\n",
    "float(sum(data_test_pred_LDA == labels_test))/float(len(labels_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
